{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CNN_MNIST.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Credits: https://github.com/keras-team/keras/blob/master/examples/mnist_cnn.py\n",
        "import keras\n",
        "from keras.datasets import mnist\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Flatten\n",
        "from keras.layers import Conv2D, MaxPooling2D\n",
        "from keras import backend as K"
      ],
      "metadata": {
        "id": "qw2DxjvGV82n"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#defining parameters for model\n",
        "batch_size = 128\n",
        "num_classes = 10\n",
        "epochs = 12"
      ],
      "metadata": {
        "id": "lxE28tZfV8pn"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IYmVjGgwXaWN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "be2b0346-e1d1-400d-aef1-991fc3a8cc35"
      },
      "source": [
        "# At first we load the data usng load_data function and the dataset is split in two parts: train set and test sets\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 0s 0us/step\n",
            "11501568/11490434 [==============================] - 0s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# input image dimensions. Since we are using MNIST dataset, the dimension of image is 28*28\n",
        "print(\"The training sample comtains {} images and the shape of image is {} \".format(x_train.shape[0] ,x_train.shape))\n",
        "print(\"The testing sample comtains {} images and the shape of image is {} \".format(x_test.shape[0] ,x_test.shape))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qePLgDjoWb3B",
        "outputId": "92db5dae-f15c-4ee4-f71e-f598de8492fa"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The training sample comtains 60000 images and the shape of image is (60000, 28, 28) \n",
            "The testing sample comtains 10000 images and the shape of image is (10000, 28, 28) \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# The image_data_format parameter affects how each of the backends treat the data dimensions when working with multi-dimensional \n",
        "# convolution layers (such as Conv2D, Conv3D, Conv2DTranspose, Copping2D, â€¦ and any other 2D or 3D layer).\n",
        "# Specifically, it defines where the 'channels' dimension is in the input data.\n",
        "\n",
        "# More on: https://www.codesofinterest.com/2017/09/keras-image-data-format.html\n",
        "img_rows= x_train.shape[1]\n",
        "img_cols= x_train.shape[2]\n",
        "if K.image_data_format() == 'channels_first':\n",
        "    x_train = x_train.reshape(x_train.shape[0], 1, img_rows, img_cols)\n",
        "    x_test = x_test.reshape(x_test.shape[0], 1, img_rows, img_cols)\n",
        "    input_shape = (1, img_rows, img_cols)\n",
        "else:\n",
        "    x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\n",
        "    x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\n",
        "    input_shape = (img_rows, img_cols, 1)\n"
      ],
      "metadata": {
        "id": "xNTv3KDOWb0o"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_train = x_train.astype('float32')\n",
        "x_test = x_test.astype('float32')\n",
        "\n",
        "#Normalizing the image pixel values\n",
        "x_train /= 255\n",
        "x_test /= 255\n",
        "\n",
        "print('x_train shape:', x_train.shape)\n",
        "print(x_train.shape[0], 'train samples')\n",
        "print(x_test.shape[0], 'test samples')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DYMC9WigWbx_",
        "outputId": "b94e3132-4207-4624-a182-e70c73ad38f3"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x_train shape: (60000, 28, 28, 1)\n",
            "60000 train samples\n",
            "10000 test samples\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#converting into 10-D vector by passing parameters-->output list, number of dimension\n",
        "# Note: The 2nd parameter in \"to_categorical\" function should contain the dimension which is equal to no. of classes  of output labels  \n",
        "from tensorflow.keras import utils\n",
        "y_train = utils.to_categorical(y_train, num_classes)\n",
        "y_test = utils.to_categorical(y_test, num_classes)"
      ],
      "metadata": {
        "id": "TdHpiPn-Wbva"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Model Building**"
      ],
      "metadata": {
        "id": "WWT3ov7AcG8p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initializing the sequential model\n",
        "model = Sequential()\n",
        "#adding the convolution layer\n",
        "model.add(Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=input_shape))\n",
        "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "model.add(Dropout(0.25))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(128, activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(num_classes, activation='softmax'))\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bHo7KabrWboV",
        "outputId": "6f49471f-e393-4481-a645-aa86952585f3"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d (Conv2D)             (None, 26, 26, 32)        320       \n",
            "                                                                 \n",
            " conv2d_1 (Conv2D)           (None, 24, 24, 64)        18496     \n",
            "                                                                 \n",
            " max_pooling2d (MaxPooling2D  (None, 12, 12, 64)       0         \n",
            " )                                                               \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 12, 12, 64)        0         \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 9216)              0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 128)               1179776   \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (None, 128)               0         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 10)                1290      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1,199,882\n",
            "Trainable params: 1,199,882\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#In model.compile we pass three main parameters. i.e optimizer, loss, and metrics.\n",
        "\n",
        "#Optimizer is an algorithm that modifies the attributes of out NN, such as weights and \n",
        "# learning rate.Thus, it helps in reducing the overall loss and improve the accuracy.\n",
        "\n",
        "#Loss: This is the objective function that the model will try to minimize.\n",
        "\n",
        "#metrics: It defines how accurate our model is. For classification task we\n",
        "#generally use 'accuracy' as evaluating metric.(There are other methods like 'confusion matrix' for assessing our model)\n",
        "\n",
        "#The actual training is performed by 'model.fit' which is used to train our convolutional neural network\n",
        "model.compile(loss=keras.losses.categorical_crossentropy, optimizer='adam', metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "wbrIpxo8Wbl9"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, verbose=1, validation_data=(x_test, y_test))\n",
        "score = model.evaluate(x_test, y_test, verbose=0)\n",
        "print('Test loss:', score[0])\n",
        "print('Test accuracy:', score[1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MwhTYrQOWbj3",
        "outputId": "913b3d28-e4b2-46a7-9332-9a08be7ec58c"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/12\n",
            "469/469 [==============================] - 16s 10ms/step - loss: 0.2369 - accuracy: 0.9278 - val_loss: 0.0545 - val_accuracy: 0.9819\n",
            "Epoch 2/12\n",
            "469/469 [==============================] - 4s 9ms/step - loss: 0.0804 - accuracy: 0.9762 - val_loss: 0.0406 - val_accuracy: 0.9871\n",
            "Epoch 3/12\n",
            "469/469 [==============================] - 4s 9ms/step - loss: 0.0612 - accuracy: 0.9818 - val_loss: 0.0331 - val_accuracy: 0.9884\n",
            "Epoch 4/12\n",
            "469/469 [==============================] - 4s 9ms/step - loss: 0.0505 - accuracy: 0.9847 - val_loss: 0.0316 - val_accuracy: 0.9896\n",
            "Epoch 5/12\n",
            "469/469 [==============================] - 4s 9ms/step - loss: 0.0436 - accuracy: 0.9863 - val_loss: 0.0308 - val_accuracy: 0.9900\n",
            "Epoch 6/12\n",
            "469/469 [==============================] - 5s 11ms/step - loss: 0.0381 - accuracy: 0.9877 - val_loss: 0.0302 - val_accuracy: 0.9897\n",
            "Epoch 7/12\n",
            "469/469 [==============================] - 4s 9ms/step - loss: 0.0325 - accuracy: 0.9899 - val_loss: 0.0295 - val_accuracy: 0.9900\n",
            "Epoch 8/12\n",
            "469/469 [==============================] - 4s 9ms/step - loss: 0.0301 - accuracy: 0.9901 - val_loss: 0.0305 - val_accuracy: 0.9910\n",
            "Epoch 9/12\n",
            "469/469 [==============================] - 4s 9ms/step - loss: 0.0276 - accuracy: 0.9910 - val_loss: 0.0273 - val_accuracy: 0.9920\n",
            "Epoch 10/12\n",
            "469/469 [==============================] - 4s 9ms/step - loss: 0.0251 - accuracy: 0.9919 - val_loss: 0.0297 - val_accuracy: 0.9907\n",
            "Epoch 11/12\n",
            "469/469 [==============================] - 4s 9ms/step - loss: 0.0221 - accuracy: 0.9928 - val_loss: 0.0271 - val_accuracy: 0.9919\n",
            "Epoch 12/12\n",
            "469/469 [==============================] - 4s 9ms/step - loss: 0.0206 - accuracy: 0.9931 - val_loss: 0.0368 - val_accuracy: 0.9903\n",
            "Test loss: 0.03675093874335289\n",
            "Test accuracy: 0.9902999997138977\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "TbjpwWPfWbhF"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}